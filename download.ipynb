{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os.path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "def crawl_url(urlFolder):\n",
    "    url = urlFolder[0]\n",
    "    folder = urlFolder[1]\n",
    "    \n",
    "    filename = folder + os.path.basename(urlparse(url).path) # use filename in url as filename in data/\n",
    "    \n",
    "    if (os.path.isfile(filename)):\n",
    "        return url, filename, None\n",
    "    \n",
    "    try:\n",
    "        fh, http_message = urlretrieve(url, filename)\n",
    "        return url, fh, None\n",
    "    except Exception as e:\n",
    "        return url, None, e\n",
    "    \n",
    "def crawl_urls(file_names):\n",
    "    print(\"Downloading\", len(file_names), \"documents\")\n",
    "    \n",
    "    start = timer()\n",
    "    results = ThreadPool(32).imap_unordered(crawl_url, file_names)\n",
    "\n",
    "    for url, fh, error in results:\n",
    "        if error is None:\n",
    "            print(\"%r ✔️ %.2fs\" % (fh, timer() - start))\n",
    "        else:\n",
    "            print(\"Error fetching %r: %s\" % (url, error))\n",
    "\n",
    "    print(\"Elapsed Time: %s\" % (timer() - start,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a Date Range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling from 2015-09-01 to 2015-09-30\n"
     ]
    }
   ],
   "source": [
    "dateRange = pd.date_range(start='2015-09-01',end='2015-09-30', freq='1D').tolist()\n",
    "print(\"Crawling from\", dateRange[0].strftime('%Y-%m-%d'), \"to\", dateRange[-1].strftime('%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBpedia Events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_name = \"http://events.dbpedia.org/dataset/%s.ttl\"\n",
    "file_names = [((file_name % ts.strftime('%Y/%m/%d')), \"dbpedia/events/\") for ts in dateRange]\n",
    "crawl_urls(file_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phoenix Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_name = \"https://s3.amazonaws.com/oeda/data/current/events.full.%s.txt.zip\"\n",
    "file_names = [((file_name % ts.strftime('%Y%m%d')), \"phoenixdata/events/\") for ts in dateRange]\n",
    "crawl_urls(file_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GDELT v1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GDELT v1 Events:\n",
    "file_name = \"http://data.gdeltproject.org/events/%s.export.CSV.zip\"\n",
    "file_names = [((file_name % ts.strftime('%Y%m%d')), \"GDELT/v1/\") for ts in dateRange]\n",
    "crawl_urls(file_names)\n",
    "\n",
    "# GDELT v1 GKG:\n",
    "file_name = \"http://data.gdeltproject.org/gkg/%s.gkg.csv.zip\"\n",
    "file_names = [((file_name % ts.strftime('%Y%m%d')), \"GDELT/v1/\") for ts in dateRange]\n",
    "crawl_urls(file_names)\n",
    "\n",
    "# GDELT v1 GKG Counts:\n",
    "file_name = \"http://data.gdeltproject.org/gkg/%s.gkgcounts.csv.zip\"\n",
    "file_names = [((file_name % ts.strftime('%Y%m%d')), \"GDELT/v1/\") for ts in dateRange]\n",
    "crawl_urls(file_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GDELT v2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# GDELT v2: WARNING: 17,265 items, totalling 63.4 GB!\n",
    "\n",
    "#http://data.gdeltproject.org/gdeltv2/masterfilelist.txt\n",
    "#http://data.gdeltproject.org/gdeltv2/masterfilelist-translation.txt\n",
    "\n",
    "gdelt2_eng = pd.read_csv(\"GDELT/v2_201509-masterfilelist.txt\", sep=' ', header=None)\n",
    "file_names = [(name, \"GDELT/v2/\") for name in gdelt2_eng[2]]\n",
    "crawl_urls(file_names)\n",
    "\n",
    "gdelt2_trans = pd.read_csv(\"GDELT/v2_201509-masterfilelist-translation.txt\", sep=' ', header=None)\n",
    "file_names = [(name, \"GDELT/v2/\") for name in gdelt2_trans[2]]\n",
    "crawl_urls(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
